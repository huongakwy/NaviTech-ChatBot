#!/usr/bin/env python3
"""
Simple E-commerce Sitemap Crawler
Ch·ªâ c·∫ßn nh·∫≠p URL ‚Üí t·ª± ƒë·ªông crawl products t·ª´ sitemap
"""
import re
import requests
import xml.etree.ElementTree as ET
from urllib.parse import urljoin, urlparse
from typing import List, Dict
import json
import pandas as pd
from bs4 import BeautifulSoup
import trafilatura
from datetime import datetime
import time
import os
from dotenv import load_dotenv

load_dotenv()


# ‚öôÔ∏è Configuration - Set c·ª©ng m·∫∑c ƒë·ªãnh
class Config:
    AI_PROVIDER = 'openai'           # openai, gemini, grok
    AI_EXTRACT_PRICES = False        # Disable AI extraction - Gemini free tier b·ªã quota limit (2 req/min)
    RATE_LIMIT_DELAY = 0.01          # Nh·ªè delay 10ms - tr√°nh b·ªã rate limit (t·ª´ 0 ‚Üí 0.01)
    TIMEOUT = 2                       # Request timeout (gi·∫£m t·ª´ 3 -> 2s - b·ªè trang ch·∫≠m)
    MAX_RETRIES = 3                   # Retry khi fail (tƒÉng t·ª´ 0 -> 3 ƒë·ªÉ handle 429)
    USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0'
    PRICE_MIN_THRESHOLD = 0           # Gi√° t·ªëi thi·ªÉu h·ª£p l·ªá
    PRICE_MAX_THRESHOLD = 1e15        # Gi√° t·ªëi ƒëa h·ª£p l·ªá
    DESCRIPTION_MAX_LENGTH = 300      # C·∫Øt description sau N k√Ω t·ª±
    # Connection pooling - aggressive cho parallel requests
    POOL_CONNECTIONS = 50            # TƒÉng t·ª´ 20 -> 50
    POOL_MAXSIZE = 50                # TƒÉng t·ª´ 20 -> 50
    RETRIES_BACKOFF = 0.5            # Backoff 500ms (tƒÉng t·ª´ 0 ƒë·ªÉ handle 429)
    # Extract optimization - ch·ªâ l·∫•y data c·∫ßn thi·∫øt
    EXTRACT_TRAFILATURA = False      # B·ªé trafilatura - qu√° ch·∫≠m, ch·ªâ d√πng JSON-LD + OG
    # Anti-blocking
    RANDOM_DELAY = (0.1, 0.5)        # Random delay 100-500ms gi·ªØa requests


class AIAgent:
    """AI Agent ƒë·ªÉ nh·∫≠n di·ªán product sitemaps"""
    
    def __init__(self, provider=None):
        """provider: 'openai', 'gemini', ho·∫∑c 'grok'"""
        self.provider = provider or Config.AI_PROVIDER
        self.api_key = self._get_api_key()
        
        # Fallback: n·∫øu API key empty, t√¨m provider kh√°c
        if not self.api_key:
            self.provider, self.api_key = self._find_available_provider()
    
    def _find_available_provider(self):
        """T√¨m provider ƒë·∫ßu ti√™n c√≥ API key"""
        providers = ['openai', 'gemini', 'grok']
        for p in providers:
            if p == 'openai':
                key = os.getenv('OPENAI_API_KEY')
            elif p == 'gemini':
                key = os.getenv('GEMINI_API_KEY') or os.getenv('GEMINI_API_KEY1') or os.getenv('GOOGLE_API_KEY')
            else:  # grok
                key = os.getenv('XAI_API_KEY')
            
            if key:
                return p, key
        
        return 'openai', None  # Fallback default (n·∫øu kh√¥ng c√≥ API key n√†o)
    
    def _get_api_key(self):
        """L·∫•y API key t·ª´ environment"""
        if self.provider == 'openai':
            return os.getenv('OPENAI_API_KEY')
        elif self.provider == 'gemini':
            return os.getenv('GEMINI_API_KEY') or os.getenv('GEMINI_API_KEY1') or os.getenv('GOOGLE_API_KEY')
        elif self.provider == 'grok':
            return os.getenv('XAI_API_KEY')
        return None
    
    def _get_gemini_keys(self):
        """L·∫•y danh s√°ch c√°c Gemini API keys ƒë·ªÉ retry"""
        keys = []
        key1 = os.getenv('GEMINI_API_KEY')
        key2 = os.getenv('GEMINI_API_KEY1')
        key3 = os.getenv('GOOGLE_API_KEY')
        
        if key1:
            keys.append(key1)
        if key2 and key2 != key1:  # Avoid duplicate
            keys.append(key2)
        if key3 and key3 not in [key1, key2]:  # Avoid duplicate
            keys.append(key3)
        
        return keys
    
    def identify_product_sitemaps(self, sitemap_urls: List[str]) -> List[str]:
        """
        D√πng AI ƒë·ªÉ nh·∫≠n di·ªán sitemap n√†o ch·ª©a products
        
        Args:
            sitemap_urls: Danh s√°ch c√°c sitemap URLs
        
        Returns:
            Danh s√°ch c√°c sitemap ch·ª©a products
        """
        if not self.api_key:
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y API key cho {self.provider}")
            print(f"   ‚Üí D√πng heuristic fallback")
            return self._heuristic_identify(sitemap_urls)
        
        print(f"ü§ñ D√πng AI ({self.provider}) ƒë·ªÉ nh·∫≠n di·ªán product sitemaps...")
        
        try:
            if self.provider == 'openai':
                return self._openai_identify(sitemap_urls)
            elif self.provider == 'gemini':
                return self._gemini_identify(sitemap_urls)
            elif self.provider == 'grok':
                return self._grok_identify(sitemap_urls)
        except Exception as e:
            print(f"‚ö†Ô∏è  AI error: {e}")
            print(f"   ‚Üí D√πng heuristic fallback")
            return self._heuristic_identify(sitemap_urls)
    
    def _heuristic_identify(self, sitemap_urls: List[str]) -> List[str]:
        """Heuristic fallback n·∫øu kh√¥ng c√≥ AI"""
        product_keywords = [
            'product', 'item', 'goods', 'san-pham',
            '_products_', 'catalog'
        ]
        
        # Exclude patterns - c√°c sitemap ch·∫Øc ch·∫Øn KH√îNG c√≥ products
        exclude_keywords = [
            'news', 'blog', 'page', 'landing',
            'collection.xml', 'collections.xml',  # Collection listing, kh√¥ng ph·∫£i products
            'category', 'categories', 'tags'
        ]
        
        product_sitemaps = []
        for url in sitemap_urls:
            url_lower = url.lower()
            
            # Lo·∫°i b·ªè exclude patterns
            if any(x in url_lower for x in exclude_keywords):
                continue
            
            # Ch·∫•p nh·∫≠n n·∫øu c√≥ product keywords
            if any(x in url_lower for x in product_keywords):
                product_sitemaps.append(url)
        
        return product_sitemaps
    
    def _openai_identify(self, sitemap_urls: List[str]) -> List[str]:
        """D√πng OpenAI API"""
        import openai
        
        client = openai.OpenAI(api_key=self.api_key)
        
        prompt = f"""B·∫°n l√† chuy√™n gia ph√¢n t√≠ch e-commerce sitemaps.

D∆∞·ªõi ƒë√¢y l√† danh s√°ch c√°c sitemap URLs t·ª´ m·ªôt trang web:

{json.dumps(sitemap_urls, indent=2)}

H√ÉY PH√ÇN T√çCH v√† CH·ªà TR·∫¢ V·ªÄ c√°c sitemap URLs c√≥ kh·∫£ nƒÉng ch·ª©a PRODUCT PAGES (trang s·∫£n ph·∫©m).

C√°c sitemap th∆∞·ªùng KH√îNG ch·ª©a products:
- news, blog, pages, landings, collections (collection listing)
- category, tags

C√°c sitemap th∆∞·ªùng ch·ª©a products:
- product, item, goods, catalog
- collection_products (products trong collection)

TR·∫¢ V·ªÄ JSON array v·ªõi format:
{{
  "product_sitemaps": ["url1", "url2", ...],
  "reasoning": "gi·∫£i th√≠ch ng·∫Øn g·ªçn"
}}"""
        
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0
        )
        
        result = json.loads(response.choices[0].message.content)
        print(f"   AI reasoning: {result.get('reasoning', '')}")
        return result.get('product_sitemaps', [])
    
    def _gemini_identify(self, sitemap_urls: List[str]) -> List[str]:
        """D√πng Google Gemini API v·ªõi retry b·∫±ng 2 keys"""
        import google.generativeai as genai
        
        gemini_keys = self._get_gemini_keys()
        
        prompt = f"""You are an e-commerce sitemap analyzer.

Here are sitemap URLs from a website:

{json.dumps(sitemap_urls, indent=2)}

ANALYZE and RETURN only the sitemap URLs that likely contain PRODUCT PAGES.

Sitemaps that usually DON'T contain products:
- news, blog, pages, landings, collections (collection listing)
- category, tags

Sitemaps that usually contain products:
- product, item, goods, catalog
- collection_products (products in collection)

RETURN JSON format:
{{
  "product_sitemaps": ["url1", "url2", ...],
  "reasoning": "brief explanation"
}}"""
        
        last_error = None
        for key_idx, api_key in enumerate(gemini_keys, 1):
            try:
                genai.configure(api_key=api_key)
                model = genai.GenerativeModel('gemini-2.5-pro')
                
                response = model.generate_content(
                    prompt,
                    generation_config=genai.GenerationConfig(
                        response_mime_type="application/json",
                        temperature=0
                    )
                )
                
                result = json.loads(response.text)
                print(f"   AI reasoning: {result.get('reasoning', '')}")
                return result.get('product_sitemaps', [])
                
            except Exception as e:
                last_error = e
                error_msg = str(e)
                
                # Check if it's a quota error
                if '429' in error_msg or 'quota' in error_msg.lower():
                    print(f"‚ö†Ô∏è  AI error (key {key_idx}): {error_msg}")
                    if key_idx < len(gemini_keys):
                        print(f"   ‚Üí Th·ª≠ key #{key_idx + 1}...")
                    else:
                        print(f"   ‚Üí H·∫øt c√°c keys, d√πng heuristic fallback")
                else:
                    # L·ªói kh√¥ng ph·∫£i quota
                    print(f"‚ùå Gemini API error (key {key_idx}): {error_msg}")
                    raise
        
        # N·∫øu h·∫øt t·∫•t c·∫£ keys, tr·∫£ v·ªÅ empty list (s·∫Ω d√πng heuristic)
        print(f"‚ö†Ô∏è  T·∫•t c·∫£ Gemini keys ƒë√£ h·∫øt quota. D√πng heuristic fallback.")
        return []
    
    def _grok_identify(self, sitemap_urls: List[str]) -> List[str]:
        """D√πng xAI Grok API"""
        
        url = "https://api.x.ai/v1/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        prompt = f"""Analyze these sitemap URLs and identify which ones contain product pages:

{json.dumps(sitemap_urls, indent=2)}

Return JSON:
{{
  "product_sitemaps": ["url1", "url2"],
  "reasoning": "explanation"
}}"""
        
        data = {
            "model": "grok-beta",
            "messages": [{"role": "user", "content": prompt}],
            "response_format": {"type": "json_object"},
            "temperature": 0
        }
        
        response = requests.post(url, headers=headers, json=data, timeout=30)
        response.raise_for_status()
        
        result = json.loads(response.json()['choices'][0]['message']['content'])
        print(f"   AI reasoning: {result.get('reasoning', '')}")
        return result.get('product_sitemaps', [])
    
    def extract_prices_with_ai(self, html: str, soup) -> dict:
        """
        D√πng AI ƒë·ªÉ t·ª± ƒë·ªông ph√¢n t√≠ch HTML v√† tr√≠ch xu·∫•t gi√°
        Detect pattern: <del>gi√° g·ªëc</del> <ins>gi√° khuy·∫øn m√£i</ins>
        """
        if not self.api_key:
            return {'price': 0, 'original_price': 0}
        
        try:
            # Extract price-related HTML snippets
            price_html = str(soup.find(['div', 'span', 'p'], class_=re.compile(r'price', re.I)))[:500]
            
            # N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c element c√≥ class 'price', d√πng to√†n b·ªô body
            if not price_html or len(price_html) < 20 or price_html == 'None':
                # L·∫•y body HTML (lo·∫°i tags script/style)
                for script in soup(["script", "style"]):
                    script.decompose()
                price_html = str(soup.body)[:1000] if soup.body else str(soup)[:1000]
            
            if not price_html or len(price_html) < 20:
                return {'price': 0, 'original_price': 0}
            
            if self.provider == 'openai':
                import openai
                client = openai.OpenAI(api_key=self.api_key)
                
                prompt = f"""Ph√¢n t√≠ch HTML n√†y v√† tr√≠ch xu·∫•t gi√° s·∫£n ph·∫©m:

HTML: {price_html}

T√¨m ki·∫øm:
1. Gi√° g·ªëc (t·ª´ <del>, strikethrough, gi√° c≈©)
2. Gi√° hi·ªán t·∫°i (t·ª´ <ins>, gi√° m·ªõi, gi√° active)

Tr·∫£ v·ªÅ JSON (ch·ªâ s·ªë, kh√¥ng k√Ω t·ª± ti·ªÅn t·ªá):
{{
  "original_price": 0,
  "current_price": 0
}}

Tr·∫£ v·ªÅ CH·ªà JSON, kh√¥ng gi·∫£i th√≠ch."""
                
                response = client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    temperature=0,
                    max_tokens=100
                )
                
                content = response.choices[0].message.content
                # Parse JSON t·ª´ markdown code block ho·∫∑c raw JSON
                if '```' in content:
                    # Extract JSON t·ª´ ```json ... ```
                    json_match = content.split('```json')[-1].split('```')[0].strip()
                else:
                    json_match = content.strip()
                
                result = json.loads(json_match)
                return {
                    'original_price': float(result.get('original_price', 0)),
                    'price': float(result.get('current_price', 0))
                }
            
            elif self.provider == 'gemini':
                import google.generativeai as genai
                genai.configure(api_key=self.api_key)
                
                prompt = f"""Ph√¢n t√≠ch HTML n√†y v√† tr√≠ch xu·∫•t gi√° s·∫£n ph·∫©m:

HTML: {price_html}

T√¨m ki·∫øm:
1. Gi√° g·ªëc (t·ª´ <del>, strikethrough, gi√° c≈©)
2. Gi√° hi·ªán t·∫°i (t·ª´ <ins>, gi√° m·ªõi, gi√° active)

Tr·∫£ v·ªÅ JSON (ch·ªâ s·ªë, kh√¥ng k√Ω t·ª± ti·ªÅn t·ªá):
{{
  "original_price": 0,
  "current_price": 0
}}

Tr·∫£ v·ªÅ CH·ªà JSON, kh√¥ng gi·∫£i th√≠ch."""
                
                model = genai.GenerativeModel('gemini-2.5-pro')
                response = model.generate_content(prompt)
                
                content = response.text
                # Parse JSON t·ª´ markdown code block ho·∫∑c raw JSON
                if '```' in content:
                    # Extract JSON t·ª´ ```json ... ```
                    json_match = content.split('```json')[-1].split('```')[0].strip()
                else:
                    json_match = content.strip()
                
                result = json.loads(json_match)
                return {
                    'original_price': float(result.get('original_price', 0)),
                    'price': float(result.get('current_price', 0))
                }
        except Exception as e:
            # Debug: log error
            import traceback
            print(f"‚ö†Ô∏è  AI extract_prices error: {e}")
            print(f"   Traceback: {traceback.format_exc()[:200]}")
        
        return {'price': 0, 'original_price': 0}


class SimpleSitemapCrawler:
    """Crawler ƒë∆°n gi·∫£n t·ª´ sitemap"""
    
    def __init__(self, base_url: str, ai_provider='openai'):
        self.base_url = base_url.rstrip('/')
        self.domain = urlparse(base_url).netloc
        self.headers = {
            'User-Agent': Config.USER_AGENT
        }
        self.ai_agent = AIAgent(provider=ai_provider)
        
        # ‚ö° Connection pooling + reuse - t·ªëi ∆∞u cho parallel requests
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry
        
        self.session = requests.Session()
        
        # Configure retry strategy
        retry_strategy = Retry(
            total=Config.MAX_RETRIES,
            backoff_factor=Config.RETRIES_BACKOFF,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "OPTIONS"]
        )
        
        # HTTPAdapter with connection pooling
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=Config.POOL_CONNECTIONS,
            pool_maxsize=Config.POOL_MAXSIZE
        )
        
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
    
    def inspect_sitemap(self):
        """Debug function: xem c·∫•u tr√∫c sitemap"""
        print(f"\nüîç INSPECT SITEMAP: {self.base_url}\n")
        
        sitemap_url = self.base_url + '/sitemap.xml'
        content = self.fetch_sitemap(sitemap_url)
        
        if not content:
            print("‚ùå Kh√¥ng t√¨m th·∫•y sitemap.xml")
            return
        
        try:
            root = ET.fromstring(content)
            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            if root.tag.endswith('sitemapindex'):
                print("üìÇ ƒê√¢y l√† SITEMAP INDEX\n")
                sitemaps = root.findall('.//sm:sitemap', ns)
                if not sitemaps:
                    sitemaps = root.findall('.//sitemap')
                
                print(f"T√¨m th·∫•y {len(sitemaps)} sub-sitemaps:\n")
                for i, sitemap in enumerate(sitemaps, 1):
                    loc = sitemap.find('sm:loc', ns)
                    if loc is None:
                        loc = sitemap.find('loc')
                    lastmod = sitemap.find('sm:lastmod', ns)
                    if lastmod is None:
                        lastmod = sitemap.find('lastmod')
                    
                    if loc is not None and loc.text:
                        url = loc.text.strip()
                        mod = lastmod.text if lastmod is not None and lastmod.text else 'N/A'
                        print(f"  {i}. {url}")
                        print(f"     Last modified: {mod}")
            
            elif root.tag.endswith('urlset'):
                print("üìÑ ƒê√¢y l√† URLSET (sitemap tr·ª±c ti·∫øp)\n")
                urls = root.findall('.//sm:url', ns)
                if not urls:
                    urls = root.findall('.//url')
                print(f"C√≥ {len(urls)} URLs\n")
                
                # Show sample
                for i, url_elem in enumerate(urls[:5], 1):
                    loc = url_elem.find('sm:loc', ns) or url_elem.find('loc')
                    if loc is not None and loc.text:
                        print(f"  {i}. {loc.text}")
        
        except Exception as e:
            print(f"‚ùå L·ªói parse: {e}")
    
    def fetch_sitemap(self, url: str) -> str:
        """T·∫£i sitemap - fallback: curl khi requests fail"""
        # Try a sequence of Accept headers to handle picky servers
        accept_values = [
            None,
            'application/xml, text/xml, */*',
            'text/xml, application/xml, */*',
            'application/rss+xml, application/xml, text/xml, */*',
            'text/plain, */*; q=0.1'
        ]

        last_error = None
        for accept in accept_values:
            try:
                headers = dict(self.headers)
                if accept:
                    headers['Accept'] = accept
                resp = self.session.get(url, headers=headers, timeout=Config.TIMEOUT)
                resp.raise_for_status()
                return resp.text
            except Exception as e:
                last_error = e
                # If it's 415 specifically, try next Accept header; otherwise continue retry logic
                err_str = str(e)
                # small backoff before next try
                time.sleep(0.1)

        # As a last resort, try curl fallback (bypass some WAFs/behavior)
        try:
            import subprocess
            curl_cmd = [
                'curl', '-s', '-L', '-A', Config.USER_AGENT,
                '-H', 'Accept: application/xml, text/xml, */*', url
            ]
            result = subprocess.run(curl_cmd, capture_output=True, text=True, timeout=15)
            if result.returncode == 0 and result.stdout:
                return result.stdout
        except Exception:
            pass

        print(f"  ‚ùå Kh√¥ng t·∫£i ƒë∆∞·ª£c {url}: {str(last_error)[:120]}")
        return ""
    
    def get_sitemap_urls(self) -> List[str]:
        """T√¨m v√† l·∫•y t·∫•t c·∫£ URLs t·ª´ sitemap"""
        
        # C√°c v·ªã tr√≠ sitemap ph·ªï bi·∫øn
        sitemap_paths = [
            '/sitemap.xml',
            '/sitemap_index.xml',
            '/sitemap-index.xml',
            '/product-sitemap.xml',
            '/products-sitemap.xml',
        ]
        
        all_urls = []
        
        for path in sitemap_paths:
            sitemap_url = self.base_url + path
            
            content = self.fetch_sitemap(sitemap_url)
            if not content:
                continue
            
            # Parse XML
            try:
                # Try lxml first (more forgiving), fallback to ElementTree
                try:
                    from lxml import etree as lxml_etree
                    parser = lxml_etree.XMLParser(recover=True)
                    root = lxml_etree.fromstring(content.encode('utf-8'), parser=parser)
                except (ImportError, Exception):
                    root = ET.fromstring(content)
                ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
                
                # Ki·ªÉm tra xem ƒë√¢y l√† sitemap index hay urlset
                if root.tag.endswith('sitemapindex'):
                    # L·∫•y c√°c sub-sitemap
                    sitemaps_found = root.findall('.//sm:sitemap', ns)
                    if not sitemaps_found:
                        sitemaps_found = root.findall('.//sitemap')
                    
                    # L·∫•y t·∫•t c·∫£ sitemap URLs
                    all_sitemap_urls = []
                    for sitemap in sitemaps_found:
                        loc = sitemap.find('sm:loc', ns)
                        if loc is None:
                            loc = sitemap.find('loc')
                        if loc is not None and loc.text:
                            all_sitemap_urls.append(loc.text.strip())
                    
                    # D√πng AI ƒë·ªÉ identify product sitemaps
                    product_sitemap_urls = self.ai_agent.identify_product_sitemaps(all_sitemap_urls)
                    
                    # Crawl c√°c product sitemaps (recursive)
                    for sub_url in product_sitemap_urls:
                        sub_urls = self._crawl_sitemap_recursive(sub_url)
                        all_urls.extend(sub_urls)
                
                elif root.tag.endswith('urlset'):
                    urls = self._parse_urlset(content)
                    all_urls.extend(urls)
                    print(f"  ‚îî‚îÄ {len(urls)} URLs")
                
                if all_urls:
                    break  # ƒê√£ t√¨m th·∫•y sitemap, kh√¥ng c·∫ßn t√¨m ti·∫øp
                    
            except Exception as e:
                error_msg = str(e)[:120]
                if 'mismatched tag' in error_msg:
                    print(f"  ‚ö†Ô∏è L·ªói parse XML: mismatched tag (c·∫Øt ng·∫Øn/malformed)")
                else:
                    print(f"  ‚ö†Ô∏è L·ªói parse XML: {error_msg}")
                continue
        
        return all_urls
    
    def _crawl_sitemap_recursive(self, sitemap_url: str, depth: int = 0, max_depth: int = 3) -> List[str]:
        """Crawl sitemap recursively ƒë·ªÉ x·ª≠ l√Ω sitemap index trong sitemap"""
        if depth > max_depth:
            return []
        
        urls = []
        content = self.fetch_sitemap(sitemap_url)
        if not content:
            return []
        
        try:
            # Try lxml first (more forgiving), fallback to ElementTree
            try:
                from lxml import etree as lxml_etree
                parser = lxml_etree.XMLParser(recover=True)
                root = lxml_etree.fromstring(content.encode('utf-8'), parser=parser)
            except (ImportError, Exception):
                root = ET.fromstring(content)
            
            # N·∫øu l√† sitemapindex -> crawl ti·∫øp v√†o c√°c sitemap con
            if root.tag.endswith('sitemapindex'):
                ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
                indent = "  " * (depth + 1)
                
                for sitemap_elem in root.findall('.//sm:sitemap', ns) or root.findall('.//sitemap'):
                    loc = sitemap_elem.find('sm:loc', ns) or sitemap_elem.find('loc')
                    if loc is not None and loc.text:
                        sub_url = loc.text.strip()
                        print(f"{indent}‚îî‚îÄ Sub-sitemap: {sub_url}")
                        sub_urls = self._crawl_sitemap_recursive(sub_url, depth + 1, max_depth)
                        urls.extend(sub_urls)
                        if sub_urls:
                            print(f"{indent}   ‚úì {len(sub_urls)} URLs")
            
            # N·∫øu l√† urlset -> parse URLs
            elif root.tag.endswith('urlset'):
                urls = self._parse_urlset(content)
        
        except Exception as e:
            error_msg = str(e)[:100]
            if depth == 0:
                if 'mismatched tag' in error_msg:
                    print(f"  ‚ö†Ô∏è XML mismatched tag (c√≥ th·ªÉ b·ªã c·∫Øt ng·∫Øn ho·∫∑c malformed)")
                else:
                    print(f"  ‚ö†Ô∏è L·ªói parse recursive: {error_msg}")
        
        return urls
    
    def _parse_urlset(self, content: str) -> List[str]:
        """Parse urlset XML - x·ª≠ l√Ω c·∫£ c√≥ v√† kh√¥ng c√≥ namespace"""
        urls = []
        try:
            # Try lxml first (more forgiving with malformed XML), fallback to ElementTree
            try:
                from lxml import etree as lxml_etree
                parser = lxml_etree.XMLParser(recover=True)  # recover=True: forgiving parser
                root = lxml_etree.fromstring(content.encode('utf-8'), parser=parser)
            except (ImportError, Exception):
                # Fallback to standard ElementTree
                root = ET.fromstring(content)
            
            ns = {'sm': 'http://www.sitemaps.org/schemas/sitemap/0.9'}
            
            # Th·ª≠ t√¨m v·ªõi namespace tr∆∞·ªõc
            url_elements = root.findall('.//sm:url', ns)
            if not url_elements:
                # N·∫øu kh√¥ng c√≥, t√¨m kh√¥ng namespace
                try:
                    url_elements = root.findall('.//{http://www.sitemaps.org/schemas/sitemap/0.9}url')
                except:
                    pass
            if not url_elements:
                # Cu·ªëi c√πng th·ª≠ kh√¥ng c√≥ namespace g√¨ c·∫£
                url_elements = root.findall('.//url')
            
            for url_elem in url_elements:
                # Th·ª≠ nhi·ªÅu c√°ch t√¨m <loc>
                loc = url_elem.find('sm:loc', ns)
                if loc is None:
                    try:
                        loc = url_elem.find('{http://www.sitemaps.org/schemas/sitemap/0.9}loc')
                    except:
                        pass
                if loc is None:
                    loc = url_elem.find('loc')
                
                if loc is not None and loc.text:
                    urls.append(loc.text.strip())
        except Exception as e:
            # Log detailed error info for debugging
            error_msg = str(e)[:100]
            if 'mismatched tag' in error_msg:
                # Try to detect truncated XML
                if not content.rstrip().endswith('>'):
                    print(f"    ‚ö†Ô∏è XML c√≥ th·ªÉ b·ªã c·∫Øt ng·∫Øn (kh√¥ng k·∫øt th√∫c b·∫±ng >)")
                else:
                    print(f"    ‚ö†Ô∏è XML c√≥ mismatched tags: {error_msg}")
            else:
                print(f"    ‚ö†Ô∏è Parse error: {error_msg}")
        return urls
    
    def is_product_url(self, url: str) -> bool:
        """Heuristic ƒë∆°n gi·∫£n ƒë·ªÉ ph√°t hi·ªán product URL"""
        
        # Lo·∫°i tr·ª´ c√°c URL kh√¥ng ph·∫£i product
        exclude_patterns = [
            '/category', '/danh-muc', '/collection',
            '/blog', '/tin-tuc', '/news',
            '/search', '/cart', '/checkout', '/account',
            '/page', '/about', '/contact', '/help',
        ]
        
        if any(pattern in url.lower() for pattern in exclude_patterns):
            return False
        
        # Product patterns
        product_patterns = [
            r'/san-pham/',
            r'/product/',
            r'/p/',
            r'-p\d+',           # tiki style
            r'/\d+\.html',      # s·ªë.html
            r'-i\d+',           # shopee/lazada style
        ]
        
        if any(re.search(pattern, url, re.I) for pattern in product_patterns):
            return True
        
        # N·∫øu URL c√≥ s·ªë ID d√†i (>= 5 digits), c√≥ th·ªÉ l√† product
        if re.search(r'\d{5,}', url):
            return True
        
        return False
    
    def _extract_description_from_html(self, soup, url: str) -> str:
        """
        Parse HTML ƒë·ªÉ t√¨m description section - h·ªó tr·ª£ c·∫•u tr√∫c HTML ƒëa d·∫°ng
        T√¨m "M√¥ t·∫£", "Description", "Chi ti·∫øt", v.v. v√† extract text ƒë·∫ßy ƒë·ªß
        Priority: HTML patterns ‚Üí Full page text parsing ‚Üí JSON-LD
        """
        description = ""
        
        # Pattern 1: C√°c pattern t√¨m heading (Ti·∫øng Vi·ªát + Ti·∫øng Anh)
        heading_patterns = [
            r'm√¥\s+t·∫£', r'description', r'chi\s+ti·∫øt\s+s·∫£n\s+ph·∫©m',
            r'th√¥ng\s+tin\s+chi\s+ti·∫øt', r's·∫£n\s+ph·∫©m\s+chi\s+ti·∫øt',
            r'product\s+details', r'about\s+product', r'th√¥ng\s+tin\s+s·∫£n\s+ph·∫©m'
        ]
        
        # Pattern 2: C√°c selector class/id ph·ªï bi·∫øn ch·ª©a description
        selectors_to_try = [
            # WooCommerce specific (popular e-commerce platform) - HIGHEST PRIORITY
            {'tag': 'div', 'attrs': {'class': re.compile(r'woocommerce-tabs-panel.*description|tab-panel.*description', re.I)}},
            # Class-based (most specific first)
            {'tag': 'div', 'attrs': {'class': re.compile(r'(description|mota|chi-tiet|detail|content|entry-content)', re.I)}},
            {'tag': 'div', 'attrs': {'id': re.compile(r'(description|mota|detail|content|main)', re.I)}},
            # itemprop
            {'tag': 'div', 'attrs': {'itemprop': 'description'}},
            # Data attributes (common in modern e-com)
            {'tag': 'div', 'attrs': {'data-section': re.compile(r'(description|detail)', re.I)}},
            # Fallback: section tag
            {'tag': 'section', 'attrs': {'class': re.compile(r'(description|detail|content)', re.I)}},
        ]
        
        # PRIORITY 2.5 (MOVED UP): Th·ª≠ CSS selectors TR∆Ø·ªöC heading patterns
        # V√¨ m·ªôt s·ªë site nh∆∞ WooCommerce ƒë√£ ƒë√≥ng g√≥i description ƒë·∫πp trong div v·ªõi class c·ª• th·ªÉ
        for selector in selectors_to_try:
            # Handle both 'attrs' dict format and direct parameters
            if 'attrs' in selector:
                tag = selector.get('tag', 'div')
                attrs = selector['attrs']
                # Convert attrs to keyword arguments
                elem = soup.find(tag, **attrs)
            else:
                elem = soup.find(**selector)
            
            if elem:
                text = elem.get_text(separator=' ', strip=True)
                # Need meaningful content (at least 200 chars for good description)
                if len(text) > 200:
                    description = text
                    break
                # If selector found but short (< 200), still use if nothing better found later
                elif len(text) > 100:
                    description = text
                    # Don't break - keep looking for longer content
        
        
        # PRIORITY 1: Th·ª≠ t√¨m t·ª´ heading pattern (n·∫øu ch∆∞a t√¨m ƒë∆∞·ª£c qua selector)
        # T√¨m heading text v·ªõi patterns nh∆∞ "M√¥ t·∫£", "Description", etc.
        if not description or len(description) < 200:
            for heading in soup.find_all(['h2', 'h3', 'h4', 'h5', 'span', 'strong', 'b', 'div']):
                heading_text = heading.get_text(strip=True)
                
                # Skip n·∫øu heading qu√° d√†i (likely navigation menu, kh√¥ng ph·∫£i heading th·ª±c)
                if len(heading_text) > 500:
                    continue
                
                if any(re.search(pattern, heading_text, re.I) for pattern in heading_patterns):
                    # Strategy 1: T√¨m parent container c√≥ th·ªÉ ch·ª©a to√†n b·ªô description
                    # M·ªôt s·ªë page (v√≠ d·ª• MYPC) gh√©p text description v√†o c√πng container v·ªõi heading
                    container = heading.find_parent(['div', 'section', 'article', 'main']) or heading
                    content_parts = []
                    
                    # Th√™m text t·ª´ heading element n·∫øu n√≥ d√†i h∆°n heading pattern
                    full_heading_text = heading.get_text(separator=' ', strip=True)
                    if len(full_heading_text) > 100:  # C√≥ th·ªÉ heading element ch·ª©a c·∫£ description
                        # Extract t·ª´ heading nh∆∞ng b·ªè ph·∫ßn heading pattern
                        clean_text = re.sub(r'^(m√¥\s+t·∫£|description|chi\s+ti·∫øt|th√¥ng\s+tin)[\s\-:]*', '', full_heading_text, flags=re.I, count=1)
                        if len(clean_text.strip()) > 50:
                            content_parts.append(clean_text)
                    
                    # T√¨m siblings
                    current = heading.find_next_sibling()
                    max_siblings = 30
                    skip_count = 0
                    
                    while current and len(content_parts) < 20 and max_siblings > 0:
                        max_siblings -= 1
                        
                        # Skip certain elements
                        if current.name in ['table', 'form', 'script', 'style', 'noscript']:
                            current = current.find_next_sibling()
                            continue
                        
                        # Stop conditions
                        if current.name and current.name.startswith('h'):  # G·∫∑p heading kh√°c = stop
                            break
                        if current.name == 'div' and current.get('class'):
                            class_str = ' '.join(current.get('class', [])).lower()
                            # Skip div containers cho specs, pricing, related
                            if any(x in class_str for x in ['spec', 'price', 'related', 'sidebar', 'nav', 'footer', 'breadcrumb']):
                                current = current.find_next_sibling()
                                continue
                        
                        # Extract text
                        if current.name in ['p', 'div', 'h2', 'h3', 'h4', 'h5', 'blockquote', 'ul', 'ol', 'li']:
                            text = current.get_text(separator=' ', strip=True)
                            # Filter out short text, pure numbers, navigation
                            if (len(text.strip()) > 20 and 
                                not re.match(r'^[0-9\.\,\-\+\s]+$', text) and
                                not any(nav in text.lower() for nav in ['ƒëƒÉng nh·∫≠p', 'ƒëƒÉng k√Ω', 'gi·ªè h√†ng', 't√¨m ki·∫øm'])):
                                content_parts.append(text)
                                skip_count = 0  # Reset skip counter
                        else:
                            skip_count += 1
                            if skip_count > 5:  # Too many non-content siblings = stop
                                break
                        
                        current = current.find_next_sibling()
                    
                    if content_parts:  # Removed requirement for >= 2 parts
                        description = ' '.join(content_parts)
                        break
                    
                    # Strategy 2: N·∫øu Strategy 1 kh√¥ng work, t√¨m t·ª´ parent container v·ªõi recursive
                    if not description or len(description) < 100:
                        container = heading.find_parent(['div', 'section', 'article']) or heading
                        content_parts = []
                        
                        for elem in container.find_all(['p', 'h2', 'h3', 'h4', 'blockquote', 'ul', 'ol', 'li'], recursive=True):
                            if elem != heading and elem not in heading.find_parents():
                                # Skip heading tags v√† tables
                                if elem.name and elem.name.startswith('h'):
                                    continue
                                if elem.name == 'table':
                                    continue
                                
                                text = elem.get_text(separator=' ', strip=True)
                                if (len(text.strip()) > 20 and 
                                    not re.match(r'^[0-9\.\,\-\+\s]+$', text)):
                                    content_parts.append(text)
                                
                                if len(content_parts) >= 20:
                                    break
                        
                        if content_parts:
                            description = ' '.join(content_parts[:20])
        

        # PRIORITY 2: N·∫øu ch∆∞a t√¨m ƒë∆∞·ª£c t·ª´ heading, parse TO√ÄN B·ªò page text
        # Lo·∫°i b·ªè script, style, nav, footer, sidebar
        if not description or len(description) < 100:
            # Clone soup ƒë·ªÉ kh√¥ng modify original
            soup_copy = BeautifulSoup(str(soup), 'html.parser')
            
            # Remove script, style, nav, footer, sidebar, etc.
            for tag in soup_copy(['script', 'style', 'meta', 'link', 'noscript', 'svg', 'path']):
                tag.decompose()
            
            # Remove common non-content elements
            try:
                for div in list(soup_copy.find_all('div')):  # Convert to list to avoid iterator issues
                    if not div or not div.name:  # Skip if decomposed
                        continue
                    class_str = ' '.join(div.get('class', [])).lower() if div.get('class') else ''
                    id_str = div.get('id', '').lower() if div.get('id') else ''
                    
                    # Skip navigation, footer, sidebar, breadcrumb, etc.
                    if any(x in (class_str + id_str) for x in ['nav', 'footer', 'sidebar', 'breadcrumb', 'menu', 'header', 'search', 'cart', 'comment', 'social', 'category', 'widget']):
                        div.decompose()
            except:
                pass  # If decomposition fails, continue
            
            # Extract all paragraphs and text
            content_parts = []
            for elem in soup_copy.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'li', 'blockquote', 'article', 'main']):
                text = elem.get_text(separator=' ', strip=True)
                
                # Skip category/navigation lists
                if any(nav in text.lower()[:100] for nav in ['danh m·ª•c', 'categories', 's·∫£n ph·∫©m', 'linh ki·ªán', 'laptop ‚Äì', 'gi·∫£m gi√°', 'khuy·∫øn m√£i']):
                    if len(text) < 500:  # Only skip if short (category list)
                        continue
                
                # Filter
                if (len(text.strip()) > 20 and 
                    not re.match(r'^[0-9\.\,\-\+\s]+$', text) and
                    not any(nav in text.lower() for nav in ['ƒëƒÉng nh·∫≠p', 'ƒëƒÉng k√Ω', 'gi·ªè h√†ng', 't√¨m ki·∫øm', 'search', 'login', 'cart'])):
                    content_parts.append(text)
            
            # L·∫•y t·ªëi ƒëa 50 ƒëo·∫°n text
            if content_parts:
                # S·∫Øp x·∫øp theo ƒë·ªô d√†i (∆∞u ti√™n nh·ªØng ƒëo·∫°n d√†i - likely content)
                content_parts = sorted(content_parts, key=len, reverse=True)[:50]
                description = ' '.join(content_parts)
        
        # PRIORITY 3: Fallback sang JSON-LD schema n·∫øu HTML kh√¥ng ƒë·ªß
        if not description or len(description) < 100:
            if hasattr(self, '_last_json_ld_desc') and self._last_json_ld_desc:
                description = self._last_json_ld_desc
        
        # Clean text
        if description:
            # Remove extra whitespace
            description = re.sub(r'\s+', ' ', description)
            # Remove breadcrumbs
            description = re.sub(r'Trang ch·ªß\s*[/>].*?:\s*', '', description, flags=re.I)
            # Remove common noise
            description = re.sub(r'(Chia s·∫ª|Share|Like|ƒê√°nh gi√°|Review|G·ª≠i b√¨nh lu·∫≠n|Li√™n h·ªá|Contact).*?$', '', description, flags=re.I)
            # Remove HTML entities if any
            description = description.replace('&nbsp;', ' ').replace('&quot;', '"').replace('&amp;', '&')
            # Clean up - trim to 5000 chars (ƒë·ªÉ l·∫•y full m√¥ t·∫£ chi ti·∫øt)
            # Note: For embedding, may need to chunk text if > 8191 tokens
            description = description.strip()[:5000]
        
        return description
    
    def _extract_category_from_html(self, soup, url: str) -> str:
        """
        Parse HTML ƒë·ªÉ t√¨m category/breadcrumb - h·ªó tr·ª£ nhi·ªÅu c·∫•u tr√∫c kh√°c nhau
        T√¨m t·ª´ breadcrumb, schema.org, URL, ho·∫∑c HTML structure
        """
        category = ""
        
        # 1. Th·ª≠ t·ª´ JSON-LD schema (category th∆∞·ªùng c√≥ trong Product schema)
        json_ld_scripts = soup.find_all('script', type='application/ld+json')
        for json_ld in json_ld_scripts:
            try:
                data = json.loads(json_ld.string)
                if isinstance(data, list):
                    for item in data:
                        if isinstance(item, dict) and item.get('@type') == 'Product':
                            data = item
                            break
                
                if isinstance(data, dict):
                    # T√¨m category t·ª´ schema
                    if 'category' in data:
                        category = data['category']
                        if isinstance(category, dict):
                            category = category.get('name', '')
                        break
                    # Ho·∫∑c t·ª´ breadcrumb
                    if 'breadcrumb' in data:
                        breadcrumb = data['breadcrumb']
                        if isinstance(breadcrumb, dict):
                            items = breadcrumb.get('itemListElement', [])
                            if items and len(items) > 1:
                                # L·∫•y category t·ª´ second-to-last breadcrumb item
                                category = items[-2].get('name', '')
                        break
            except:
                pass
        
        if category:
            return category.strip()
        
        # 2. Th·ª≠ t·ª´ breadcrumb HTML structure
        breadcrumb_selectors = [
            {'tag': 'nav', 'attrs': {'class': re.compile(r'breadcrumb', re.I)}},
            {'tag': 'div', 'attrs': {'class': re.compile(r'breadcrumb', re.I)}},
            {'tag': 'ol', 'attrs': {'class': re.compile(r'breadcrumb', re.I)}},
            {'tag': 'ul', 'attrs': {'class': re.compile(r'breadcrumb', re.I)}},
        ]
        
        for selector in breadcrumb_selectors:
            breadcrumb = soup.find(**selector)
            if breadcrumb:
                # T√¨m t·∫•t c·∫£ links ho·∫∑c items trong breadcrumb
                items = breadcrumb.find_all(['a', 'li', 'span'])
                if items and len(items) >= 2:
                    # Th∆∞·ªùng category l√† item g·∫ßn cu·ªëi (tr∆∞·ªõc product name)
                    for item in reversed(items[:-1]):
                        text = item.get_text(strip=True)
                        # B·ªè "Home", "Trang ch·ªß", numbers, etc.
                        if text and len(text) > 2 and not re.match(r'^\d+$', text):
                            if text.lower() not in ['home', 'trang ch·ªß', 'all', 'shop']:
                                category = text
                                break
                if category:
                    break
        
        if category:
            return category.strip()
        
        # 3. Th·ª≠ t·ª´ URL path
        # Pattern: /category/subcategory/product-name
        from urllib.parse import urlparse
        path = urlparse(url).path
        path_parts = [p for p in path.split('/') if p and p not in ['product', 'products']]
        
        if path_parts:
            # L·∫•y part g·∫ßn cu·ªëi (tr∆∞·ªõc product slug)
            # Th∆∞·ªùng c√≥ pattern: domain/category/subcategory/product-name
            if len(path_parts) >= 2:
                # L·∫•y second-to-last part
                category_slug = path_parts[-2]
                # Convert slug to readable format
                category = category_slug.replace('-', ' ').title()
            elif len(path_parts) == 1:
                category = path_parts[0].replace('-', ' ').title()
        
        # 4. Th·ª≠ t·ª´ structured data attributes
        if not category:
            category_elem = soup.find(attrs={'itemtype': re.compile(r'Product|Thing', re.I)})
            if category_elem:
                category_meta = category_elem.find(attrs={'itemprop': 'category'})
                if category_meta:
                    category = category_meta.get_text(strip=True)
        
        # 5. Th·ª≠ t·ª´ meta tags
        if not category:
            cat_meta = soup.find('meta', attrs={'name': re.compile(r'category|product-category', re.I)})
            if cat_meta:
                category = cat_meta.get('content', '').strip()
        
        # Clean category
        if category:
            # Remove extra whitespace
            category = re.sub(r'\s+', ' ', category)
            # Remove special chars
            category = category.replace('|', ' ').replace('>', ' ').strip()
            # Take first category if multiple separated by comma
            if ',' in category:
                category = category.split(',')[0].strip()
        
        return category.strip() if category else ""
    
    def _extract_original_price_from_html(self, soup, current_price: float) -> float:
        """
        Parse HTML ƒë·ªÉ t√¨m gi√° g·ªëc/gi√° khuy·∫øn m√£i
        Khi JSON-LD kh√¥ng c√≥ priceBefore, t√¨m t·ª´:
        - Strikethrough text (<del>, <s>, <strike>)
        - Old price badges
        - Discount percentage (t√≠nh ng∆∞·ª£c t·ª´ discount %)
        """
        original_price = 0
        
        if current_price <= 0:
            return 0
        
        # STRATEGY 1: T√¨m strikethrough price (<del>, <s>, <strike>)
        for tag in soup.find_all(['del', 's', 'strike']):
            text = tag.get_text(strip=True)
            # Extract numbers
            numbers = re.findall(r'\d+(?:[.,]\d{3})*(?:[.,]\d{2})?', text.replace('.', '').replace(',', ''))
            if numbers:
                try:
                    price = float(numbers[-1])  # L·∫•y s·ªë cu·ªëi (likely price)
                    if price > current_price and price < 1e15:  # Gi√° g·ªëc > gi√° hi·ªán t·∫°i
                        original_price = price
                        break
                except:
                    pass
        
        # STRATEGY 2: T√¨m "Old price" / "Original price" text nearby current price
        if not original_price:
            # Find price elements
            price_patterns = [
                {'class': re.compile(r'(price|gi√°|gia|cost)', re.I)},
                {'itemprop': 'price'},
                {'data-price': True}
            ]
            
            for pattern in price_patterns:
                price_elem = soup.find(attrs=pattern)
                if not price_elem:
                    continue
                
                # Look at siblings and nearby elements
                container = price_elem.find_parent(['div', 'section', 'article']) or price_elem
                
                for elem in container.find_all(['span', 'div', 'p']):
                    text = elem.get_text(strip=True)
                    
                    # Check if contains "old", "g·ªëc", "g·ªëc", "original"
                    if any(keyword in text.lower() for keyword in ['old price', 'original price', 'gi√° g·ªëc', 'gi√° c≈©', 'gi√° khuy·∫øn m√£i tr∆∞·ªõc']):
                        numbers = re.findall(r'\d+(?:[.,]\d{3})*(?:[.,]\d{2})?', text.replace('.', '').replace(',', ''))
                        if numbers:
                            try:
                                price = float(numbers[-1])
                                if price > current_price and price < 1e15:
                                    original_price = price
                                    break
                            except:
                                pass
        
        # STRATEGY 3: T√¨m discount percentage v√† t√≠nh ng∆∞·ª£c gi√° g·ªëc
        if not original_price:
            # T√¨m text nh∆∞ "-20%", "Gi·∫£m 30%", etc.
            discount_text = soup.get_text()
            
            # Extract discount percentage
            discount_matches = re.findall(r'-(\d+)%|(?:gi·∫£m|sale|discount)\s*(\d+)\s*%', discount_text, re.I)
            
            if discount_matches:
                for match in discount_matches:
                    discount_pct = int(match[0] or match[1])
                    
                    # T√≠nh gi√° g·ªëc t·ª´ c√¥ng th·ª©c: current_price = original_price * (1 - discount_pct/100)
                    # => original_price = current_price / (1 - discount_pct/100)
                    if discount_pct > 0 and discount_pct < 99:
                        calc_original = current_price / (1 - discount_pct / 100)
                        
                        # Sanity check: gi√° g·ªëc h·ª£p l√Ω (kh√¥ng qu√° 10x gi√° hi·ªán t·∫°i)
                        if calc_original > current_price and calc_original < current_price * 10:
                            original_price = calc_original
                            break
        
        # STRATEGY 4: T√¨m trong price lists (multiple prices shown)
        if not original_price:
            # T√¨m t·∫•t c·∫£ numbers gi·ªëng ki·ªÉu gi√° trong page
            price_numbers = re.findall(r'\d+(?:[.,]\d{3})*(?:[.,]\d{2})?', soup.get_text())
            
            # Convert to floats
            prices = []
            for num_str in price_numbers:
                try:
                    price = float(num_str.replace('.', '').replace(',', ''))
                    if price > current_price * 0.5 and price < 1e15:  # Filter reasonable range
                        prices.append(price)
                except:
                    pass
            
            # Take largest price as likely original
            if prices:
                max_price = max(prices)
                if max_price > current_price and max_price < current_price * 5:
                    original_price = max_price
        
        return original_price
    
    def _extract_price_from_element(self, elem, elem_type="div/span"):
        """
        Helper method: Extract price t·ª´ HTML element
        Support multiple e-commerce formats:
        - "249,000ƒë" ‚Üí 249000 (VND with currency)
        - "25.990.000‚Ç´" ‚Üí 25990000 (European format)
        - "$19.99" ‚Üí 19.99 (USD)
        - "‚Ç¨15,50" ‚Üí 15.50 (Euro)
        
        Returns: float (price) ho·∫∑c None
        """
        if not elem:
            return None
        
        price_text = elem.get_text(strip=True)
        if not price_text:
            return None
        
        # Pattern 1: Try to extract price BEFORE currency symbol
        # Matches: "249,000ƒë", "$19.99", "‚Ç¨15,50", "‚Ç´25.990.000"
        match = re.search(r'(\d+(?:[.,]\d{3})*)\s*[ƒë$‚Ç¨‚Ç´¬•]', price_text)
        if match:
            price_str = match.group(1).replace('.', '').replace(',', '')
            try:
                return float(price_str)
            except:
                pass
        
        # Pattern 2: Fallback - remove everything after currency symbol
        # Remove currency symbols and everything after them, then extract numbers
        price_text_clean = re.sub(r'[ƒë$‚Ç¨‚Ç´¬•].*', '', price_text)
        numbers = re.findall(r'\d+', price_text_clean.replace('.', '').replace(',', ''))
        if numbers:
            try:
                return float(''.join(numbers))
            except:
                pass
        
        return None
    
    def extract_product(self, url: str) -> Dict:
        """Crawl v√† extract th√¥ng tin t·ª´ 1 product page - T·ªîNG QU√ÅT cho m·ªçi site"""
        
        product = {
            'url': url,
            'title': '',
            'price': 0,
            'original_price': 0,
            'currency': 'VND',
            'sku': '',
            'brand': '',
            'category': '',
            'images': [],
            'description': '',
            'availability': ''
        }
        
        try:
            # Random delay ƒë·ªÉ tr√°nh pattern detection
            import random
            if Config.RANDOM_DELAY:
                delay = random.uniform(*Config.RANDOM_DELAY)
                time.sleep(delay)
            elif Config.RATE_LIMIT_DELAY > 0:
                time.sleep(Config.RATE_LIMIT_DELAY)
            
            try:
                resp = self.session.get(url, headers=self.headers, timeout=Config.TIMEOUT)
                resp.raise_for_status()
                html = resp.text
            except Exception as e:
                # Fallback to curl khi requests fail
                if '429' in str(e):
                    import subprocess
                    try:
                        result = subprocess.run(
                            ['curl', '-s', '-L', '-A', Config.USER_AGENT, url],
                            capture_output=True,
                            text=True,
                            timeout=Config.TIMEOUT
                        )
                        if result.returncode == 0 and result.stdout:
                            html = result.stdout
                        else:
                            raise e
                    except:
                        raise e
                else:
                    raise e
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # 1. Extract JSON-LD Schema.org (chu·∫©n e-commerce to√†n c·∫ßu)
            json_ld_scripts = soup.find_all('script', type='application/ld+json')
            for json_ld in json_ld_scripts:
                try:
                    data = json.loads(json_ld.string)
                    
                    # X·ª≠ l√Ω n·∫øu l√† @graph structure
                    if isinstance(data, dict) and '@graph' in data:
                        for item in data['@graph']:
                            if isinstance(item, dict) and item.get('@type') == 'Product':
                                data = item
                                break
                    
                    # X·ª≠ l√Ω n·∫øu l√† array
                    elif isinstance(data, list):
                        for item in data:
                            if isinstance(item, dict) and item.get('@type') == 'Product':
                                data = item
                                break
                    
                    # Ch·ªâ process n·∫øu l√† Product schema
                    if isinstance(data, dict) and data.get('@type') == 'Product':
                        product['title'] = data.get('name', '')
                        product['sku'] = data.get('sku', '')
                        product['description'] = data.get('description', '')
                        # Save ƒë·ªÉ d√πng trong helper function
                        if product['description']:
                            self._last_json_ld_desc = product['description']
                        
                        # Brand
                        brand = data.get('brand', {})
                        if isinstance(brand, dict):
                            product['brand'] = brand.get('name', '')
                        elif isinstance(brand, str):
                            product['brand'] = brand
                        
                        # Category t·ª´ schema n·∫øu c√≥
                        if 'category' in data:
                            cat = data.get('category', '')
                            if isinstance(cat, dict):
                                product['category'] = cat.get('name', '')
                            else:
                                product['category'] = cat
                        
                        # Images
                        images = data.get('image', [])
                        image_urls = []
                        if isinstance(images, str):
                            image_urls = [images]
                        elif isinstance(images, list):
                            for img in images:
                                if isinstance(img, dict) and 'url' in img:
                                    # Extract URL t·ª´ ImageObject
                                    image_urls.append(img['url'])
                                elif isinstance(img, str):
                                    image_urls.append(img)
                        product['images'] = image_urls[:5]  # L·∫•y max 5 ·∫£nh
                        
                        # Price t·ª´ offers
                        offers = data.get('offers', {})
                        if isinstance(offers, list) and offers:
                            offers = offers[0]
                        
                        if isinstance(offers, dict):
                            # Extract price
                            price_str = str(offers.get('price', '')).replace(',', '').replace('.', '')
                            if price_str:
                                try:
                                    product['price'] = float(price_str)
                                except:
                                    pass
                            
                            # Extract original price (ƒë·ªÉ detect khuy·∫øn m√£i)
                            if 'priceBefore' in offers:
                                orig_price_str = str(offers.get('priceBefore', '')).replace(',', '').replace('.', '')
                                if orig_price_str:
                                    try:
                                        product['original_price'] = float(orig_price_str)
                                    except:
                                        pass
                            elif offers.get('sameAs'):
                                # M·ªôt s·ªë site d√πng sameAs cho original price
                                pass
                            
                            product['currency'] = offers.get('priceCurrency', 'VND')
                            
                            # Availability - convert URL sang text d·ªÖ ƒë·ªçc
                            avail = offers.get('availability', '')
                            if 'InStock' in avail:
                                product['availability'] = 'C√≤n h√†ng'
                            elif 'OutOfStock' in avail:
                                product['availability'] = 'H·∫øt h√†ng'
                            elif 'PreOrder' in avail:
                                product['availability'] = 'ƒê·∫∑t tr∆∞·ªõc'
                            else:
                                product['availability'] = avail
                        
                        break  # ƒê√£ t√¨m th·∫•y Product schema
                except:
                    continue
            
            # 2. Fallback: Extract t·ª´ meta tags
            if not product['title']:
                og_title = soup.find('meta', property='og:title')
                if og_title:
                    product['title'] = og_title.get('content', '')
            
            # Fallback cho SKU - th·ª≠ extract t·ª´ URL ho·∫∑c HTML
            if not product['sku']:
                # Pattern 1: SKU trong URL (v√≠ d·ª•: --s250711016)
                sku_match = re.search(r'--s(\d+)', url)
                if sku_match:
                    product['sku'] = sku_match.group(1)
                else:
                    # Pattern 2: SKU trong HTML
                    sku_patterns = [
                        soup.find('span', {'itemprop': 'sku'}),
                        soup.find(string=re.compile(r'SKU[:|\s]', re.I)),
                        soup.find(attrs={'data-sku': True})
                    ]
                    for elem in sku_patterns:
                        if elem:
                            if hasattr(elem, 'get_text'):
                                sku_text = elem.get_text(strip=True)
                            elif hasattr(elem, 'strip'):
                                sku_text = elem.strip()
                            else:
                                sku_text = str(elem)
                            # Extract SKU number
                            sku_num = re.search(r'[\d]+', sku_text)
                            if sku_num:
                                product['sku'] = sku_num.group(0)
                                break
            
            # Fallback cho Brand - th·ª≠ extract t·ª´ title ho·∫∑c HTML
            if not product['brand']:
                # Pattern 1: Brand c√≥ th·ªÉ ·ªü ƒë·∫ßu title
                brand_patterns = [
                    soup.find('span', {'itemprop': 'brand'}),
                    soup.find('a', {'class': re.compile(r'brand', re.I)}),
                    soup.find('div', {'class': re.compile(r'brand', re.I)})
                ]
                for elem in brand_patterns:
                    if elem:
                        brand_text = elem.get_text(strip=True)
                        if brand_text and len(brand_text) < 50:
                            product['brand'] = brand_text
                            break
                
                # Pattern 2: Extract t·ª´ ƒë·∫ßu title (th∆∞·ªùng brand ƒë·ª©ng ƒë·∫ßu)
                if not product['brand'] and product['title']:
                    # L·∫•y t·ª´ ƒë·∫ßu ti√™n trong title (th∆∞·ªùng l√† brand)
                    first_word = product['title'].split()[0] if product['title'].split() else ''
                    # Ch·ªâ l·∫•y n·∫øu l√† ch·ªØ in hoa ho·∫∑c CamelCase (ƒëi·ªÉn h√¨nh c·ªßa brand)
                    if first_word and (first_word.isupper() or (first_word[0].isupper() and any(c.isupper() for c in first_word[1:]))):
                        product['brand'] = first_word
                else:
                    title_tag = soup.find('title')
                    if title_tag:
                        product['title'] = title_tag.get_text(strip=True)
            
            if product['price'] == 0:
                # T√¨m price meta tag
                price_meta = soup.find('meta', property='product:price:amount')
                if price_meta:
                    try:
                        product['price'] = float(price_meta.get('content', '0'))
                    except:
                        pass
            
            if not product['images']:
                og_image = soup.find('meta', property='og:image')
                if og_image:
                    product['images'] = [og_image.get('content', '')]
            
            # 3. Fallback cu·ªëi: T√¨m price trong HTML (common patterns)
            if product['price'] == 0:
                price_patterns = [
                    {'class': 'price-detail'},  # Rabity.vn specific
                    {'class': re.compile(r'(price|gi√°|gia)', re.I)},
                    {'itemprop': 'price'},
                    {'data-price': True}
                ]
                
                for pattern in price_patterns:
                    price_elem = soup.find(attrs=pattern)
                    if price_elem:
                        price_text = price_elem.get_text(strip=True)
                        # Extract s·ªë tr∆∞·ªõc "ƒë" ho·∫∑c "$" (VD: "249,000ƒë0ƒë-50%" ‚Üí 249000, "25.990.000‚Ç´" ‚Üí 25990000)
                        # First, extract price before currency symbols
                        match = re.search(r'(\d+(?:[.,]\d{3})*)\s*[ƒë$‚Ç¨‚Ç´]', price_text)
                        if match:
                            price_str = match.group(1).replace('.', '').replace(',', '')
                            try:
                                product['price'] = float(price_str)
                                break
                            except:
                                pass
                        else:
                            # Fallback: extract all numbers, remove those after currency
                            price_text_clean = re.sub(r'[ƒë$‚Ç¨‚Ç´].*', '', price_text)
                            numbers = re.findall(r'\d+', price_text_clean.replace('.', '').replace(',', ''))
                            if numbers:
                                try:
                                    product['price'] = float(''.join(numbers))
                                    break
                                except:
                                    pass
            
            # 3.5 Detect strikethrough/discount patterns - WooCommerce + other e-commerce
            # C√°ch ph·ªï bi·∫øn: <del>gi√° g·ªëc</del> <ins>gi√° m·ªõi</ins> ho·∫∑c <s>gi√° g·ªëc</s> <price>gi√° m·ªõi</price>
            # Lu√¥n ch·∫°y ƒë·ªÉ t√¨m original_price, ngay c·∫£ khi ƒë√£ c√≥ price
            if True:  # Lu√¥n detect ƒë·ªÉ t√¨m original_price
                try:
                    # Pattern 1: <del> + <ins>
                    del_tags = soup.find_all('del')
                    ins_tags = soup.find_all('ins')
                    
                    if del_tags and ins_tags:
                        # T√¨m original_price t·ª´ <del> n·∫øu ch∆∞a c√≥
                        if product['original_price'] == 0:
                            for del_tag in del_tags[:3]:
                                price_text = del_tag.get_text(strip=True)
                                if re.search(r'\d{3,}', price_text):
                                    numbers = re.findall(r'\d+', price_text)
                                    if numbers:
                                        try:
                                            potential_orig = float(''.join(numbers))
                                            if 100000 < potential_orig < 1e10:
                                                product['original_price'] = potential_orig
                                                break
                                        except:
                                            pass
                        
                        # T√¨m current price t·ª´ <ins> n·∫øu ch∆∞a c√≥
                        if product['price'] == 0:
                            for ins_tag in ins_tags[:3]:
                                price_text = ins_tag.get_text(strip=True)
                                if re.search(r'\d{3,}', price_text):
                                    numbers = re.findall(r'\d+', price_text)
                                    if numbers:
                                        try:
                                            potential_price = float(''.join(numbers))
                                            if 100000 < potential_price < 1e10:
                                                product['price'] = potential_price
                                                break
                                        except:
                                            pass
                    
                    # Pattern 2: <s> (strikethrough) = original price
                    if product['original_price'] == 0:
                        s_tags = soup.find_all('s')
                        for s_tag in s_tags[:3]:
                            price_text = s_tag.get_text(strip=True)
                            if re.search(r'\d{3,}', price_text):
                                numbers = re.findall(r'\d+', price_text)
                                if numbers:
                                    try:
                                        potential_orig = float(''.join(numbers))
                                        if 100000 < potential_orig < 1e10:
                                            product['original_price'] = potential_orig
                                            break
                                    except:
                                        pass
                except:
                    pass
            
            # 3.5.5 AGGRESSIVE HTML PRICE EXTRACTION - For websites like Rabity
            # T√¨m t·∫•t c·∫£ numbers c√≥ 5+ digits trong page (likely prices)
            if product['price'] == 0:
                try:
                    # Extract t·∫•t c·∫£ text t·ª´ page
                    page_text = soup.get_text()
                    # Find all numbers with 5+ digits (likely prices)
                    all_numbers = re.findall(r'\d{5,}', page_text.replace(',', '').replace('.', ''))
                    
                    if all_numbers:
                        # Convert to floats v√† filter by reasonable price range
                        potential_prices = []
                        for num_str in all_numbers:
                            try:
                                num = float(num_str)
                                if 50000 < num < 1e10:  # Reasonable price range (50k - 10 billion VND)
                                    potential_prices.append(num)
                            except:
                                pass
                        
                        if potential_prices:
                            # Th∆∞·ªùng gi√° ƒë·∫ßu ti√™n hay th·∫•y l√† gi√° hi·ªán t·∫°i
                            product['price'] = min(potential_prices)
                except:
                    pass
            
            # 3.6 AI Fallback - d√πng AI ƒë·ªÉ ph√¢n t√≠ch pattern <del>/<ins> n·∫øu ch∆∞a c√≥ original_price
            # Ch·∫°y ƒë·ªÉ t√¨m original_price ngay c·∫£ khi ƒë√£ c√≥ price (c√≥ th·ªÉ kh√¥ng ph·∫£i gi√° g·ªëc)
            if Config.AI_EXTRACT_PRICES and self.ai_agent and product['original_price'] == 0:
                try:
                    ai_prices = self.ai_agent.extract_prices_with_ai(html, soup)
                    # N·∫øu AI t√¨m ƒë∆∞·ª£c original_price, h√£y d√πng n√≥
                    if ai_prices['original_price'] > 0:
                        product['original_price'] = ai_prices['original_price']
                    # N·∫øu ch∆∞a c√≥ price nh∆∞ng AI t√¨m ƒë∆∞·ª£c, d√πng gi√° t·ª´ AI
                    elif ai_prices['price'] > 0 and product['price'] == 0:
                        product['price'] = ai_prices['price']
                except Exception as e:
                    pass  # Silent fail
            
            # 3.7 HTML Fallback - Parse HTML ƒë·ªÉ t√¨m gi√° g·ªëc n·∫øu JSON-LD kh√¥ng c√≥
            # T∆∞∆°ng t·ª± description extraction - d√πng strikethrough, discount badge, etc.
            if product['original_price'] == 0 and product['price'] > 0:
                html_original_price = self._extract_original_price_from_html(soup, product['price'])
                if html_original_price > 0:
                    product['original_price'] = html_original_price
            
            # 4. Extract description chi ti·∫øt t·ª´ HTML
            # Always try to get full description from page - override JSON-LD short snippet
            extracted_desc = self._extract_description_from_html(soup, url)
            if extracted_desc and len(extracted_desc) > len(product.get('description', '')):
                product['description'] = extracted_desc
            
            # 5. Extract category t·ª´ HTML (breadcrumb, schema, URL, etc.)
            if not product['category']:
                extracted_cat = self._extract_category_from_html(soup, url)
                if extracted_cat:
                    product['category'] = extracted_cat
            
            # Final cleanup
            if product['description']:
                desc = product['description']
                desc = re.sub(r'Trang ch·ªß\s*/[^:]+:\s*', '', desc, flags=re.I)
                # Removed overly aggressive regex that was truncating descriptions
                product['description'] = desc.strip()
        
        except Exception as e:
            print(f"    ‚ö†Ô∏è L·ªói: {str(e)[:50]}")
        
        return product
    
    def crawl(self, max_products: int = 10000):
        """Main crawl function - t·ªëi ∆∞u h√≥a cho t·ªëc ƒë·ªô"""
        
        # L·∫•y URLs t·ª´ sitemap
        all_urls = self.get_sitemap_urls()
        
        if not all_urls:
            return []
        
        product_urls = all_urls
        
        if not product_urls:
            return []
        
        # Crawl products - kh√¥ng print chi ti·∫øt
        products = []
        for i, url in enumerate(product_urls[:max_products], 1):
            product = self.extract_product(url)
            products.append(product)
        
        return products
        print(f"  C√≥ images: {sum(1 for p in products if p['images'])}")
        
        print("\n" + "="*70)
        print("‚úÖ HO√ÄN TH√ÄNH!")
        print("="*70)


if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("C√°ch d√πng: python3 crawl.py <URL> [s·ªë_s·∫£n_ph·∫©m] [--ai=openai|gemini|grok]")
        print("           python3 crawl.py <URL> --inspect  (xem c·∫•u tr√∫c sitemap)")
        print("\nV√≠ d·ª•:")
        print("  python3 crawl.py https://phongvu.vn")
        print("  python3 crawl.py https://phongvu.vn --ai=gemini")
        print("  python3 crawl.py https://phongvu.vn 50 --ai=openai")
        print("  python3 crawl.py https://phongvu.vn --inspect")
        print("\nAI Providers:")
        print("  openai  ‚Üí C·∫ßn OPENAI_API_KEY")
        print("  gemini  ‚Üí C·∫ßn GEMINI_API_KEY ho·∫∑c GOOGLE_API_KEY")
        print("  grok    ‚Üí C·∫ßn XAI_API_KEY")
        print("\nN·∫øu kh√¥ng c√≥ API key, s·∫Ω d√πng heuristic fallback")
        sys.exit(1)
    
    url = sys.argv[1]
    
    # Parse arguments
    ai_provider = 'openai'
    max_products = 10000  # M·∫∑c ƒë·ªãnh crawl h·∫øt t·∫•t c·∫£ s·∫£n ph·∫©m
    inspect_mode = False
    
    for arg in sys.argv[2:]:
        if arg == '--inspect':
            inspect_mode = True
        elif arg.startswith('--ai='):
            ai_provider = arg.split('=')[1]
        elif arg.isdigit():
            max_products = int(arg)
    
    crawler = SimpleSitemapCrawler(url, ai_provider=ai_provider)
    
    if inspect_mode:
        crawler.inspect_sitemap()
    else:
        crawler.crawl(max_products)
