from fastapi import APIRouter, UploadFile, File, Form, HTTPException, Body
from pydantic import BaseModel, HttpUrl
from typing import Optional
import sys
import os
import shutil
from datetime import datetime

# Add AI_crawl to path ƒë·ªÉ import pipeline
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AI_crawl'))

router = APIRouter(
    prefix="/upload",
    tags=["File Upload"]
)

# Upload directory
UPLOAD_DIR = os.path.join(os.path.dirname(__file__), '..', 'uploads')
os.makedirs(UPLOAD_DIR, exist_ok=True)


class ProductUploadResponse(BaseModel):
    """Response model for product file upload"""
    status: str
    message: str
    filename: str
    products_count: int
    timestamp: str
    task_id: str


class DocumentUploadResponse(BaseModel):
    """Response model for document file upload"""
    status: str
    message: str
    filename: str
    chunks_count: int
    timestamp: str
    task_id: str


@router.post("/product", response_model=ProductUploadResponse)
async def upload_product_file(
    file: UploadFile = File(...),
    user_id: str = Form(...),  # Required - no default
    website_name: Optional[str] = Form(None)
) -> ProductUploadResponse:
    """
    Upload file s·∫£n ph·∫©m v√† import v√†o database + embeddings
    
    **üìÅ Supported formats:**
    - **Excel**: .xlsx, .xls (v·ªõi t√™n c·ªôt kh·ªõp database)
    - **CSV**: .csv (v·ªõi t√™n c·ªôt kh·ªõp database)
    - **JSON**: .json (v·ªõi field kh·ªõp database)
    - **Word**: .docx (tables t·ª± ƒë·ªông parse, ho·∫∑c AI extract n·∫øu kh√¥ng c√≥ table)
    - **PDF**: .pdf (tables t·ª± ƒë·ªông parse, ho·∫∑c AI extract n·∫øu kh√¥ng c√≥ table)
    
    **üìã C·∫•u tr√∫c d·ªØ li·ªáu (t√™n c·ªôt/field ph·∫£i kh·ªõp):**
    
    **B·∫Øt bu·ªôc:**
    - `url` - Link URL s·∫£n ph·∫©m (UNIQUE)
    - `title` ho·∫∑c `name` - T√™n s·∫£n ph·∫©m
    
    **T√πy ch·ªçn (NULL n·∫øu kh√¥ng c√≥):**
    - `price`, `gi√°` - Gi√° b√°n
    - `original_price`, `gi√° g·ªëc` - Gi√° g·ªëc
    - `currency` - ƒê∆°n v·ªã ti·ªÅn t·ªá (default: VND)
    - `sku`, `m√£ s·∫£n ph·∫©m` - M√£ SKU
    - `brand`, `th∆∞∆°ng hi·ªáu` - Th∆∞∆°ng hi·ªáu
    - `category`, `danh m·ª•c` - Danh m·ª•c
    - `description`, `m√¥ t·∫£` - M√¥ t·∫£
    - `availability` - T√¨nh tr·∫°ng
    - `images`, `h√¨nh ·∫£nh` - Link ·∫£nh (CSV: ph√¢n c√°ch b·∫±ng d·∫•u ph·∫©y)
    
    **ü§ñ AI Extraction:**
    - N·∫øu file kh√¥ng c√≥ table/c·∫•u tr√∫c r√µ r√†ng ‚Üí t·ª± ƒë·ªông d√πng Gemini AI extract
    - AI s·∫Ω extract theo ƒë√∫ng schema database
    
    Args:
        - **file**: File upload
        - **user_id**: User ID (UUID) - default: hardcoded
        - **website_name**: T√™n website (optional, d√πng filename n·∫øu kh√¥ng c√≥)
    
    Returns:
        ProductUploadResponse v·ªõi products_count sau khi ho√†n th√†nh
    """ 
    # Validate file extension
    allowed_extensions = ['.xlsx', '.xls', '.csv', '.json', '.docx', '.pdf']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}"
        )
    
    # Validate file size (max 50MB)
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    file.file.seek(0, 2)
    file_size = file.file.tell()
    file.file.seek(0)
    
    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"File too large: {file_size / (1024*1024):.1f}MB. Max: 50MB"
        )
    
    # Generate task ID
    task_id = f"product_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    file_path = os.path.join(UPLOAD_DIR, f"{task_id}{file_ext}")
    
    # Save uploaded file
    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving file: {str(e)}")
    
    # Import product data
    try:
        from AI_crawl.pipeline import import_from_file
        
        products_count = import_from_file(
            file_path=file_path,
            user_id=user_id,
            website_name=website_name
        )
        
        return ProductUploadResponse(
            status="success",
            message=f"Product file uploaded and processed successfully",
            filename=file.filename,
            products_count=products_count or 0,
            timestamp=datetime.now().isoformat(),
            task_id=task_id
        )
        
    except Exception as e:
        # Clean up file on error
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")
    
    finally:
        file.file.close()


@router.post("/document", response_model=DocumentUploadResponse)
async def upload_document_file(
    file: UploadFile = File(...),
    user_id: str = Form("b95c2881-387f-4093-b80d-ac83b1dea7a9"),
    document_name: Optional[str] = Form(None)
) -> DocumentUploadResponse:
    """
    Upload vƒÉn b·∫£n text ƒë·ªÉ chatbot h·ªçc (policies, FAQs, guides, etc.)
    
    Supported formats:
    - **Text**: .txt
    - **Word**: .docx (text content)
    - **PDF**: .pdf (text content)
    
    Args:
        - **file**: File upload
        - **user_id**: User ID (UUID) - default: hardcoded
        - **document_name**: T√™n document (optional, d√πng filename n·∫øu kh√¥ng c√≥)
    
    Returns:
        DocumentUploadResponse v·ªõi chunks_count sau khi ho√†n th√†nh
    """ 
    # Validate file extension
    allowed_extensions = ['.txt', '.docx', '.pdf']
    file_ext = os.path.splitext(file.filename)[1].lower()
    
    if file_ext not in allowed_extensions:
        raise HTTPException(
            status_code=400,
            detail=f"Unsupported file type: {file_ext}. Allowed: {', '.join(allowed_extensions)}"
        )
    
    # Validate file size (max 50MB)
    MAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB
    file.file.seek(0, 2)
    file_size = file.file.tell()
    file.file.seek(0)
    
    if file_size > MAX_FILE_SIZE:
        raise HTTPException(
            status_code=400,
            detail=f"File too large: {file_size / (1024*1024):.1f}MB. Max: 50MB"
        )
    
    # Generate task ID
    task_id = f"document_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    file_path = os.path.join(UPLOAD_DIR, f"{task_id}{file_ext}")
    
    # Save uploaded file
    try:
        with open(file_path, "wb") as buffer:
            shutil.copyfileobj(file.file, buffer)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error saving file: {str(e)}")
    
    # Import text document
    try:
        from AI_crawl.pipeline import import_text_document
        
        chunks_count = import_text_document(
            file_path=file_path,
            user_id=user_id,
            document_name=document_name or file.filename
        )
        
        return DocumentUploadResponse(
            status="success",
            message=f"Text document uploaded and processed successfully",
            filename=file.filename,
            chunks_count=chunks_count or 0,
            timestamp=datetime.now().isoformat(),
            task_id=task_id
        )
        
    except Exception as e:
        # Clean up file on error
        if os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"Error processing file: {str(e)}")
    
    finally:
        file.file.close()


class GoogleSheetsUploadRequest(BaseModel):
    """Request model for Google Sheets link upload"""
    sheet_url: str
    user_id: str = "b95c2881-387f-4093-b80d-ac83b1dea7a9"
    website_name: Optional[str] = None


@router.post("/product/google-sheets", response_model=ProductUploadResponse)
async def upload_product_from_google_sheets(
    request: GoogleSheetsUploadRequest
) -> ProductUploadResponse:
    """
    Import s·∫£n ph·∫©m t·ª´ Google Sheets link
    
    **üìã C·∫•u tr√∫c b·∫£ng Google Sheets (c√°c t√™n c·ªôt ph·∫£i kh·ªõp v·ªõi database):**
    
    **C·ªôt b·∫Øt bu·ªôc:**
    - `url` - Link URL c·ªßa s·∫£n ph·∫©m (UNIQUE, B·∫ÆT BU·ªòC)
    - `title` ho·∫∑c `name` - T√™n s·∫£n ph·∫©m (B·∫ÆT BU·ªòC)
    
    **C·ªôt t√πy ch·ªçn (ƒë·ªÉ tr·ªëng = NULL):**
    - `price` ho·∫∑c `gi√°` - Gi√° b√°n (s·ªë, VD: 100000)
    - `original_price` ho·∫∑c `gi√° g·ªëc` - Gi√° g·ªëc (s·ªë)
    - `currency` ho·∫∑c `ƒë∆°n v·ªã` - ƒê∆°n v·ªã ti·ªÅn t·ªá (VD: VND, USD) - m·∫∑c ƒë·ªãnh: VND
    - `sku` ho·∫∑c `m√£ s·∫£n ph·∫©m` - M√£ SKU
    - `brand` ho·∫∑c `th∆∞∆°ng hi·ªáu` - Th∆∞∆°ng hi·ªáu
    - `category` ho·∫∑c `danh m·ª•c` - Danh m·ª•c s·∫£n ph·∫©m
    - `description` ho·∫∑c `m√¥ t·∫£` - M√¥ t·∫£ chi ti·∫øt
    - `availability` - T√¨nh tr·∫°ng (C√≤n h√†ng/H·∫øt h√†ng)
    - `images` ho·∫∑c `h√¨nh ·∫£nh` - Link ·∫£nh (nhi·ªÅu ·∫£nh c√°ch nhau b·∫±ng d·∫•u ph·∫©y)
    
    **L∆∞u √Ω:**
    - T√™n c·ªôt kh√¥ng ph√¢n bi·ªát hoa th∆∞·ªùng
    - T√™n c·ªôt c√≥ th·ªÉ l√† ti·∫øng Anh ho·∫∑c ti·∫øng Vi·ªát (c√≥ d·∫•u/kh√¥ng d·∫•u)
    - H·ªá th·ªëng t·ª± ƒë·ªông map t√™n c·ªôt v·ªõi database schema
    
    **üîó C√°ch l·∫•y link Google Sheets:**
    1. M·ªü Google Sheets
    2. File ‚Üí Share ‚Üí Get link
    3. Set quy·ªÅn: "Anyone with the link can view"
    4. Copy link d·∫°ng: `https://docs.google.com/spreadsheets/d/[SHEET_ID]/edit`
    
    Args:
        - **sheet_url**: Link Google Sheets (public)
        - **user_id**: User ID (UUID)
        - **website_name**: T√™n website (optional)
    
    Returns:
        ProductUploadResponse v·ªõi products_count sau khi ho√†n th√†nh
    """
    try:
        # Validate Google Sheets URL
        if "docs.google.com/spreadsheets" not in request.sheet_url:
            raise HTTPException(
                status_code=400,
                detail="Invalid Google Sheets URL. Must be from docs.google.com/spreadsheets"
            )
        
        # Extract sheet ID from URL
        import re
        match = re.search(r'/spreadsheets/d/([a-zA-Z0-9-_]+)', request.sheet_url)
        if not match:
            raise HTTPException(
                status_code=400,
                detail="Cannot extract Sheet ID from URL. Please check the link format."
            )
        
        sheet_id = match.group(1)
        
        # Generate task ID
        task_id = f"gsheet_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Convert to CSV export URL
        csv_url = f"https://docs.google.com/spreadsheets/d/{sheet_id}/export?format=csv"
        
        # Download CSV to temp file
        import requests
        response = requests.get(csv_url)
        
        if response.status_code != 200:
            raise HTTPException(
                status_code=400,
                detail=f"Cannot access Google Sheets. Please ensure the sheet is public (Anyone with link can view). Status: {response.status_code}"
            )
        
        # Save to temp CSV file
        file_path = os.path.join(UPLOAD_DIR, f"{task_id}.csv")
        with open(file_path, 'wb') as f:
            f.write(response.content)
        
        # Import product data using existing pipeline
        from AI_crawl.pipeline import import_from_file
        
        products_count = import_from_file(
            file_path=file_path,
            user_id=request.user_id,
            website_name=request.website_name
        )
        
        return ProductUploadResponse(
            status="success",
            message=f"Google Sheets imported successfully",
            filename=f"Google Sheets ({sheet_id})",
            products_count=products_count or 0,
            timestamp=datetime.now().isoformat(),
            task_id=task_id
        )
        
    except HTTPException:
        raise
    except Exception as e:
        # Clean up file on error
        if 'file_path' in locals() and os.path.exists(file_path):
            try:
                os.remove(file_path)
            except:
                pass
        raise HTTPException(status_code=500, detail=f"Error processing Google Sheets: {str(e)}")


if __name__ == "__main__":
    print("File upload endpoints ready!")
    print(f"Upload directory: {UPLOAD_DIR}")
    print(f"\nüì¶ Product Endpoint: POST /api/upload/product")
    print(f"   Formats: Excel, CSV, JSON, Word (tables), PDF (tables)")
    print(f"\nüìÑ Document Endpoint: POST /api/upload/document")
    print(f"   Formats: TXT, Word (text), PDF (text)")
    print(f"\nüîó Google Sheets Endpoint: POST /api/upload/product/google-sheets")
    print(f"   Input: Google Sheets URL (public)")

